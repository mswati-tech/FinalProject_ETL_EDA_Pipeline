# -*- coding: utf-8 -*-
"""FinalProject_code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-rZ3ux3GE8wX3ygRXVkXQ_rCHqQNh_Hh
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install ipython-sql

# Commented out IPython magic to ensure Python compatibility.
# %load_ext sql

!pip install pymongo sqlite-utils

!pip install pymongo
from pymongo import MongoClient
client = MongoClient("mongodb://localhost:27017/")
db = client["test_db"]

!pip install pymongo nltk

# Installing dependencies
#!pip install "pymongo[srv]==3.12"
#!pip install sqlite-utils
#!pip install faker
#!pip install pymongo nltk
#!pip install pandas matplotlib seaborn
#!pip install gensim pyLDAvis wordcloud

# Importing and downloading dependencies
import sqlite3
from faker import Faker
import random
from datetime import datetime, timedelta
from pymongo import MongoClient
import nltk
nltk.download('vader_lexicon')
from nltk.sentiment import SentimentIntensityAnalyzer
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import nltk
nltk.download('stopwords')
import re
nltk.download('wordnet')

from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
import string
from gensim.corpora import Dictionary
from gensim.models import LdaModel

NUM_TOPICS = 3 # Number of topics for LDA modeling

import pyLDAvis
import pyLDAvis.gensim

from wordcloud import WordCloud

from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB

# Defining a generic function to randomly distribute reviews
def generate_review_sentence():
  positive_reviews = [
        "Absolutely loved this product, the quality exceeded expectations.",
        "Really happy with this purchase, totally worth the money.",
        "Great product! Arrived early and works perfectly.",
        "Amazing quality and fast delivery. Highly recommend it!",
        "Fantastic experience, will definitely buy again."
    ]

  negative_reviews = [
        "Terrible quality, very disappointed with this product.",
        "Stopped working after a few days, not worth the price.",
        "Really poor build quality, would not recommend.",
        "Arrived damaged and customer service was unhelpful.",
        "Not satisfied, the product did not match the description."
    ]

  neutral_reviews = [
        "Overall okay, nothing special but it works fine.",
        "The product is average, decent for the price.",
        "It's acceptable, but I expected better.",
        "Quality is okay, but the delivery was slow.",
        "Neutral experience, neither great nor terrible."
    ]

  return random.choice(positive_reviews + negative_reviews + neutral_reviews)

# Creating object for Faker module
fake = Faker()

# CREATING STRUCTURED DATABASE IN SQL

try:
  # Since I don't have a database file with a .db extension, I will be creating a SQLite database in memory.
  conn = sqlite3.connect(":memory:")

  # Create the cursor obj through the connection to SQLite 3
  cursor = conn.cursor()

  # Checkpoint
  print("Connection to SQLITE established successfully.")

except sqlite3.Error as e:
  print("Connection to SQLITE failed:", e)

try:
  # Creating the structured table with 'cursor' methods

  # Table <Users>
  cursor.execute('''
  CREATE TABLE Users (
    user_id INTEGER PRIMARY KEY AUTOINCREMENT,
    name TEXT NOT NULL,
    email TEXT UNIQUE NOT NULL,
    region TEXT
  )
  ''')

  # Table <Products>
  cursor.execute('''
  CREATE TABLE Products (
    product_id INTEGER PRIMARY KEY AUTOINCREMENT,
    name TEXT NOT NULL,
    category TEXT,
    price REAL
  )
  ''')

  #Table <Orders>
  cursor.execute('''
  CREATE TABLE Orders (
    order_id INTEGER PRIMARY KEY AUTOINCREMENT,
    user_id INTEGER,
    product_id INTEGER,
    order_date TEXT,
    quantity INTEGER,
    total_amount REAL,
    FOREIGN KEY(user_id) REFERENCES Users(user_id),
    FOREIGN KEY(product_id) REFERENCES Prodcuts(product_id)
  )
  ''')

  # Commit the tables to the database
  conn.commit()

  # Checkpoint
  print("Tables created successfully")

except sqlite3.Error as e:
  print("Tables not created, failed", e)

try:
  # Inserting 10 users
  users = [(fake.name(), fake.email(), fake.country()) for _ in range(10)]
  cursor.executemany("INSERT INTO Users (name, email, region) VALUES (?,?,?)", users)

  # Inserting 10 products
  categories = ["Electronics", "Fashion", "Decor", "Books", "Beauty"]
  products = [(fake.word().capitalize(), random.choice(categories), round(random.uniform(10,500),2)) for _ in range(10)]
  cursor.executemany("INSERT INTO Products (name, category, price) VALUES (?,?,?)", products)

  # Inserting 50 orders
  order_data = []
  for _ in range(50):
    user_id = random.randint(1,10)
    product_id = random.randint(1,10)
    quantity = random.randint(1,3)
    order_date = (datetime.now() - timedelta(days=random.randint(1,180))).strftime("%Y-%m-%d")
    price = cursor.execute("SELECT price FROM Products WHERE product_id = ?", (product_id,)).fetchone()[0]
    total = round(price*quantity,2)
    order_data.append((user_id, product_id, order_date, quantity, total))

  cursor.executemany("INSERT INTO Orders (user_id, product_id, order_date, quantity, total_amount) VALUES (?,?,?,?,?)", order_data)

  # Commiting the values to the database
  conn.commit()

  print("Data inserted into tables successfully")

except sqlite3.Error as e:
  print("Data insertion into tables failed")

# Showcasing and verifying the tables
try:
  cursor.execute("SELECT COUNT(*) FROM Orders")
  count = cursor.fetchone()[0]
  print(f"Total orders inserted: {count}")

except sqlite3.error as e:
  print("The first query failed to execute", e)

# CREATING UNSTRUCTURED DATABASE IN MONGODB

try:
  # Connecting to MongoDB Atlas
  uri = "mongodb+srv://<username>:<password>@cluster0.umou3gr.mongodb.net/?appName=Cluster0"
  client = MongoClient(uri, tlsAllowInvalidCertificates=True)
  db = client["ecommerce_db"]

  # Collections
  reviews = db["reviews"]
  user_logs = db["user_logs"]

  try:
    # Inserting sample reviews
    sample_reviews = []
    for _ in range(30):
      sample_reviews.append(
      {
          "user_id":random.randint(1,10),
          "product_id":random.randint(1,10),
          "ratings":random.randint(1,5),
          "review_text":fake.sentence(),
          "timestamp":datetime.now()
      }
  )

    reviews.insert_many(sample_reviews)

    # Inserting user logs (views, add_to_cart etc.)
    actions = ["view", "add_to_cart", "purchase"]
    user_logs.insert_many([
    {
        "user_id":random.randint(1,10),
        "product_id":random.randint(1,10),
        "action":random.choice(actions),
        "timestamp":datetime.now() - timedelta(days=random.randint(0,30))
    }
      for _ in range(100)
  ])

    print("MongoDB populated with data successfully.")

  except Exceptions as e:
    print("Error inserting user logs",e)

except Exception as conn_e:
  print("MongoDB connection failed",conn_e)

#ANALYZING SENTIMENT USING VADER

try:
  sia = SentimentIntensityAnalyzer()  #Initializing Vader object
  print("VADER sentiment analyzer loaded")

except Exception as e:
  print("Failed to load VADER")

try:
  all_reviews = list(reviews.find({}, {"_id":1, "review_text":1}))
  updates = []

  for r in all_reviews:
    text = r.get("review_text", "")
    if not isinstance(text, str) or not text.strip():
      continue

    scores = sia.polarity_scores(text)
    compound = scores["compound"]

    sentiment = (
        "POSITIVE" if compound >= 0.05 else
        "NEGATIVE" if compound <= -0.05 else
        "NEUTRAL"
    )

    updates.append(
        UpdateOne(
            {"_id": r["_id"]},
            {"$set": {
                "sentiment": sentiment,
                "compound_score": compound,
                "vader_scores": scores,
                "analyzed_at": datetime.now()
            }}
        )
    )

  if updates:
    reviews.bulk_write(updates)

except Exception as e:
  print("Error analyzing reviews:", e)

print("Vader analysis complete and saved to MongoDB")

# Loading SQL Table into Pandas
products_df = pd.read_sql_query("SELECT * FROM Products", conn)
users_df = pd.read_sql_query("SELECT * FROM Users", conn)
orders_df = pd.read_sql_query("SELECT * FROM Orders", conn)

print("Products table:")
print(products_df.head())

# Loading MongoDB reviews into pandas

reviews_df = pd.DataFrame(list(reviews.find()))

if "review_text" not in reviews_df.columns:
  raise ValueError("review_text column missing from MongoDB data!")

# Ensuring product_id is integer
reviews_df["product_id"] = reviews_df["product_id"].astype(int)

print("MongoDB reviews:")
print(reviews_df.head())

# Merging SQL products with MongoDB Reviews
# Merging products and reviews
merged_df = reviews_df.merge(
    products_df,
    on="product_id",
    how="left"   # keeping all reviews, even if product missing (rare)
)

# Merging SQL users also
merged_df = merged_df.merge(
    users_df,
    on="user_id",
    how="left"
)

merged_df.rename(columns={
    "name_x": "product_name",
    "name_y": "user_name"
}, inplace=True)

print("Merged with users:")
print(merged_df.head())

# Fixing Datatypes and Cleaning any inconsistent instance
merged_df["sentiment"] = merged_df["sentiment"].fillna("UNKNOWN")
merged_df["compound_score"] = merged_df["compound_score"].fillna(0.0)

# Final Merged Structure
print("Merged dataset shape:", merged_df.shape)
print("Columns:", merged_df.columns.tolist())
merged_df.head()

print(products_df["category"].value_counts())

# Categorywise Sentiment Distribution Chart
plt.figure(figsize=(10,5))
sns.countplot(data=merged_df, x="category", hue="sentiment")
plt.title("Category-wise Sentiment Distribution")
plt.xticks(rotation=30)
plt.show()

# Productwise Sentiment Distribution Chart
plt.figure(figsize=(12,5))
sns.countplot(data=merged_df, x="product_id", hue="sentiment")
plt.title("Product-wise Sentiment Distribution")
plt.show()

try:
  # LDA/ TOPIC MODELLING
  # Preprocessing text
  stop_words = set(stopwords.words("english"))
  lemmatizer = WordNetLemmatizer()

  def preprocess_text(text):
    if not isinstance(text, str):
      return []

    tokens = re.findall(r"\b[a-zA-Z]+\b", text.lower())

    tokens = [
        lemmatizer.lemmatize(t)
        for t in tokens
        if t not in stop_words and len(t) > 2
    ]

    return tokens

  # Applying preprocessing
  merged_df["clean_tokens"] = merged_df["review_text"].apply(preprocess_text)

  print("\nSample cleaned tokens:")
  print(merged_df["clean_tokens"].head())

  # Removing empty documents
  merged_df = merged_df[merged_df["clean_tokens"].map(len) > 0]

  if merged_df.empty:
    raise ValueError("All cleaned tokens are empty; LDA cannot run.")

  # Preparing Dictionary & Corpus for LDA
  # Creating dictionary and corpus
  dictionary = Dictionary(merged_df["clean_tokens"])
  corpus = [dictionary.doc2bow(text) for text in merged_df["clean_tokens"]]

  print("\nDictionary size:", len(dictionary))

  # Training the LDA model
  lda_model = LdaModel(
        corpus=corpus,
        id2word=dictionary,
        num_topics=NUM_TOPICS,
        random_state=42,
        passes=10,
        alpha='auto'
    )

  print("LDA Model created.")

  # Printing top words per topic
  for idx, topic in lda_model.print_topics(num_topics=NUM_TOPICS, num_words=8):
    print(f"\n Topic #{idx}:")
    print(topic)

  #pyLDAvis Interactive Visualization
  pyLDAvis.enable_notebook()

  lda_vis = pyLDAvis.gensim.prepare(lda_model, corpus, dictionary)
  display(lda_vis)

  # Visualizing WordCloud for each topic
  for t in range(NUM_TOPICS):
    plt.figure(figsize=(6,4))
    plt.title(f"Topic #{t} Wordcloud")

    words = dict(lda_model.show_topic(t, 30))

    wc = WordCloud(width=800, height=400).generate_from_frequencies(words)
    plt.imshow(wc, interpolation="bilinear")
    plt.axis("off")
    plt.show()

  # Assigning dominant topic to each review
  def get_dominant_topic(bow):
    topics = lda_model.get_document_topics(bow)
    topics = sorted(topics, key=lambda x: x[1], reverse=True)
    return topics[0][0]

  merged_df["dominant_topic"] = [get_dominant_topic(bow) for bow in corpus]

  print("\nDominant topic assignment complete!")
  print(merged_df[["review_text", "dominant_topic"]].head())

except Exception as lda_error:
  print("\n LDA ERROR:", lda_error)

# MACHINE LEARINING MODELING FOR CATEGORY RECOMMENDATION
try:
  print("\nRunning Recommendation ML Model...")

  # Using only sentiment + rating to predict best category
  rec_df = merged_df[["ratings", "compound_score", "category"]].dropna()

  if rec_df.empty:
    raise ValueError("Not enough data for recommendation model.")

  # Encoding category labels
  le = LabelEncoder()
  rec_df["category_encoded"] = le.fit_transform(rec_df["category"])

  # Features (ratings & sentiment score)
  X = rec_df[["ratings", "compound_score"]]
  y = rec_df["category_encoded"]

  # Train-test splitting
  X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42
    )

  # Classifier
  model = GaussianNB()
  model.fit(X_train, y_train)

  # Predicting best category for a hypothetical user:
  # Example: high rating history & positive sentiment
  sample_user = [[5, 0.8]]  # rating=5, sentiment positive

  prediction = model.predict(sample_user)[0]
  recommended_category = le.inverse_transform([prediction])[0]

  print("\nRecommended Product Category:", recommended_category)

except Exception as rec_error:
  print("\nRecommendation Model Error:", rec_error)

!pip install "pymongo[srv]==3.12"

from pymongo import MongoClient

uri = "mongodb+srv://swatimishralko19_db_user:TSbfOJXe9w0zkeTL@cluster0.umou3gr.mongodb.net/?appName=Cluster0"
client = MongoClient(uri, tlsAllowInvalidCertificates=True)

!pip install gensim pyLDAvis wordcloud